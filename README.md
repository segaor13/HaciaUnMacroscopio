# Readme

Este repositorio acompaña la tesis doctoral ***Hacia un macroscopio. Desarrollo metodológico para una historia digital de problemas sociales en la producción noticiosa de 'El Siglo de Torreón' (2013 – 2022)***, presentada por Sergio Garza Orellana en la Universidad Iberoamericana Torreón, bajo la dirección del Dr. Jairo Melo Florez. 

El propósito de este repositorio es compartir de manera abierta y reproducible los códigos utilizados y los principales resultados visuales derivados del análisis computacional aplicado a un corpus de más de 900,000 noticias del periódico *El Siglo de Torreón* durante la década de 2013 a 2022. 

---

## Contenido

### Códigos
El desarrollo técnico de esta investigación se estructuró en cuatro módulos principales, cada uno de los cuales cumple una función específica dentro del flujo de procesamiento y análisis del corpus hemerográfico. El **1.1** implementa técnicas de *webscraping* en Python, combinando Selenium y BeautifulSoup, para automatizar la recolección de titulares y enlaces de noticias del sitio *El Siglo de Torreón*, y su posterior almacenamiento en una base de datos MySQL. El **1.2** expande este proceso al nivel del contenido completo de cada nota: a partir de los enlaces recopilados, se extraen campos estructurados como sección, balazo, autor, primer párrafo y cuerpo, registrando también los casos fallidos. El **1.3** constituye el núcleo del análisis textual; realiza el preprocesamiento del corpus (tokenización, lematización, generación de n-gramas, filtrado por Tf-Idf), el modelado de tópicos mediante LDA, la selección del modelo óptimo con base en su coherencia semántica, y la etiquetación temática de cada noticia. Finalmente, el **1.4** introduce una capa adicional de interpretación mediante la *clusterización* de los tópicos obtenidos: agrupa temáticamente los tópicos más cercanos utilizando K-Means y Tf-Idf, lo que permite identificar supertópicos como unidades de sentido más amplias. Cada uno de estos módulos está documentado en cuadernos de Jupyter y corresponde a una etapa esencial del dispositivo metodológico que articula esta historia digital de los problemas sociales en la región de La Laguna.
### Visualizaciones PyLDAvis
Cada uno de los modelos generados fue visualizado mediante la herramienta **PyLDAvis**, que permite una exploración interactiva de los tópicos obtenidos a partir del modelado LDA. Esta visualización facilita la comprensión de la estructura temática del corpus al proyectar los tópicos en un espacio bidimensional a través de técnicas de reducción de dimensionalidad. Los círculos representan tópicos y su tamaño relativo indica su prevalencia en el conjunto de datos, mientras que su distancia refleja su grado de disimilitud semántica. Al seleccionar un tópico, se despliega un listado de palabras clave ordenadas por relevancia, lo que permite interpretar el contenido de cada grupo temático con mayor precisión. Esta herramienta fue utilizada de manera sistemática para documentar y contrastar los resultados anuales y seccionales del análisis. Cada modelo está acompañado de una visualización interactiva exportada como HTML. 

### Optimización de número *k*
Para la generación de cada modelo de tópicos, se recurrió a diferentes iteraciones del número *k*, iniciando con un mínimo de 2 posibles tópicos y un máximo de 60. El algoritmo itera sumando 4 unidades a *k*, dando como resultado 15 posibles modelos para cada segmento mes/sección. Cada uno de ellos fue evaluado mediante su puntaje de coherencia: un índice que permite identificar qué tan cercanas o distantes son las relaciones de cohesión entre las palabras clave de los tópicos resultantes. El índice tiene un rango de 0 a 1, y generalmente los resultados mayores a .4 son marcados como utilizables, deseando un óptimo mayor a .6. Generalmente, los resultados con una evaluación menor a este rango son tópicos inconexos; pero un número demasiado alto (cercano a 1) también podría indicar algún error en la metodología. De entre los 15 modelos resultantes para cada mes/sección, el algoritmo creado seleccionó el modelo con un mayor puntaje de coherencia. Aunque la selección fue automatizada, se generó una gráfica cruzando los datos de *k* y el puntaje de coherencia de cada uno de los modelos. En este apartado se incluyen las gráficas que cruzan el número *k* con el puntaje de coherencia, dando como resultado el modelo más óptimo que fue utilizado en la investigación